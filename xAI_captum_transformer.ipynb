{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUp2W8CAvloDxFnV0fIwd1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescopatane96/protein-xAI/blob/main/xAI_captum_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t3zh8yWpiTV_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as Data\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "import os\n",
        "import time\n",
        "from sklearn.metrics import auc, roc_curve, average_precision_score, precision_recall_curve\n",
        "from termcolor import colored\n",
        "import pdb\n",
        "\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(file):\n",
        "    # Amino acid dictionary\n",
        "    aa_dict = {'A': 1, 'R': 2, 'N': 3, 'D': 4, 'C': 5, 'Q': 6, 'E': 7, 'G': 8, 'H': 9, 'I': 10,\n",
        "               'L': 11, 'K': 12, 'M': 13, 'F': 14, 'P': 15, 'O': 16, 'S': 17, 'U': 18, 'T': 19,\n",
        "               'W': 20, 'Y': 21, 'V': 22, 'X': 23}\n",
        "    # open csv file \n",
        "    with open(file, 'r') as inf:\n",
        "        lines = inf.read().splitlines()\n",
        "\n",
        "    pep_codes = []\n",
        "    labels = []\n",
        "    peps = []\n",
        "    \n",
        "    for pep in lines:                           # for every row\n",
        "        pep, label = pep.split(\",\")             # sequence and label split\n",
        "        peps.append(pep)\n",
        "        labels.append(int(label))\n",
        "        current_pep = []\n",
        "        for aa in pep:\n",
        "            current_pep.append(aa_dict[aa])\n",
        "        pep_codes.append(torch.tensor(current_pep))\n",
        "\n",
        "        \n",
        "\n",
        "    data = rnn_utils.pad_sequence(pep_codes, batch_first=True)  # Fill the sequence to the same length\n",
        "  \n",
        "\n",
        "    return data, torch.tensor(labels)"
      ],
      "metadata": {
        "id": "EkEdfEzHid9C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, label = generate_data(\"./SSP_dataset.csv\")\n",
        "\n",
        "train_data, train_label= data[:1894], label[:1894]   #I primi 1894 sono usati per il training\n",
        "test_data, test_label = data[1894:], label[1894:]\n",
        "\n",
        "train_dataset = Data.TensorDataset(train_data, train_label)\n",
        "test_dataset = Data.TensorDataset(test_data, test_label)\n",
        "\n",
        "batch_size = 64\n",
        "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "SbUPb4xrifyr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lunghezza delle proteine x fase di inferenza\n",
        "\n",
        "len(data[45])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-vpJa8SrEMn",
        "outputId": "17633a62-330d-4566-f699-13f081434e1b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "299"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class xAInet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.hidden_dim = 25\n",
        "    self.batch_size = 32\n",
        "    self.embedding_dim = 512\n",
        "\n",
        "    self.embedding_layer = nn.Embedding(24, self.embedding_dim, padding_idx=0)\n",
        "    self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
        "\n",
        "    self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
        "    self.gru = nn.GRU(self.embedding_dim, self.hidden_dim, num_layers=2,\n",
        "                      bidirectional=True, dropout=.2)\n",
        "    \n",
        "    self.block_seq = nn.Sequential(nn.Linear(15050, 2048),\n",
        "                                   nn.BatchNorm1d(2048),\n",
        "                                   nn.LeakyReLU(),\n",
        "                                   nn.Linear(2048, 1024),\n",
        "                                   nn.BatchNorm1d(1024),\n",
        "                                   nn.LeakyReLU(),\n",
        "                                   nn.Linear(1024, 256),\n",
        "                                   nn.BatchNorm1d(256),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Linear(256, 8),\n",
        "                                   nn.Linear(8, 2))\n",
        "    \n",
        "    \n",
        "  def forward(self, seq):\n",
        "        seq = seq.long()\n",
        "        embeddings = self.embedding_layer(seq)\n",
        "        output = self.transformer_encoder(embeddings).permute(1, 0, 2)\n",
        "        output, hn = self.gru(output)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        hn = hn.permute(1, 0, 2)\n",
        "       \n",
        "        output = output.reshape(output.shape[0], -1)\n",
        "        hn = hn.reshape(output.shape[0], -1)\n",
        "        \n",
        "        output = torch.cat([output, hn], 1)\n",
        "        output = self.block_seq(output)\n",
        "        \n",
        "        #output = F.softmax(output, dim=0)\n",
        "\n",
        "        return output\n",
        "\n",
        "  def train_model(self, seq):\n",
        "    #with torch.no_grad():\n",
        "        output = self.forward(seq)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "bvKNJ008igZg"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
        "        loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
        "                                      label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "\n",
        "        return loss_contrastive"
      ],
      "metadata": {
        "id": "5wB5vdMsii3S"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate(batch):\n",
        "    seq1_ls = []\n",
        "    seq2_ls = []\n",
        "    label1_ls = []\n",
        "    label2_ls = []\n",
        "    label_ls = []\n",
        "\n",
        "\n",
        "    batch_size = len(batch)\n",
        "    for i in range(int(batch_size / 2)):\n",
        "        seq1, label1= batch[i][0], batch[i][1]\n",
        "        seq2, label2= batch[i + int(batch_size / 2)][0], \\\n",
        "                                       batch[i + int(batch_size / 2)][1], \\\n",
        "                                       \n",
        "        label1_ls.append(label1.unsqueeze(0))\n",
        "        label2_ls.append(label2.unsqueeze(0))\n",
        "        label = (label1 ^ label2)\n",
        "        seq1_ls.append(seq1.unsqueeze(0))\n",
        "        seq2_ls.append(seq2.unsqueeze(0))\n",
        "        label_ls.append(label.unsqueeze(0))\n",
        "\n",
        "        \n",
        "\n",
        "    seq1 = torch.cat(seq1_ls).to(device)\n",
        "    seq2 = torch.cat(seq2_ls).to(device)\n",
        "\n",
        "    \n",
        "\n",
        "    label = torch.cat(label_ls).to(device)\n",
        "    label1 = torch.cat(label1_ls).to(device)\n",
        "    label2 = torch.cat(label2_ls).to(device)\n",
        "    return seq1, seq2, label, label1, label2\n",
        "\n",
        "\n",
        "train_iter_cont = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                              shuffle=True, collate_fn=collate)\n",
        "\n",
        "device = torch.device(\"cuda\", 0)\n",
        "\n",
        "\n",
        "def evaluate(data_iter, net):\n",
        "    pred_prob = []\n",
        "    label_pred = []\n",
        "    label_real = []\n",
        "    for x, y in data_iter:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = net.train_model(x)\n",
        "        outputs_cpu = outputs.cpu()\n",
        "        y_cpu = y.cpu()\n",
        "        pred_prob_positive = outputs_cpu[:, 1]\n",
        "        pred_prob = pred_prob + pred_prob_positive.tolist()\n",
        "        label_pred = label_pred + outputs.argmax(dim=1).tolist()\n",
        "        label_real = label_real + y_cpu.tolist()\n",
        "    performance, roc_data, prc_data = caculate_metric(pred_prob, label_pred, label_real)\n",
        "    return performance, roc_data, prc_data\n",
        "\n",
        "\n",
        "def caculate_metric(pred_prob, label_pred, label_real):\n",
        "    test_num = len(label_real)\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    for index in range(test_num):\n",
        "        if label_real[index] == 1:\n",
        "            if label_real[index] == label_pred[index]:\n",
        "                tp = tp + 1\n",
        "            else:\n",
        "                fn = fn + 1\n",
        "        else:\n",
        "            if label_real[index] == label_pred[index]:\n",
        "                tn = tn + 1\n",
        "            else:\n",
        "                fp = fp + 1\n",
        "\n",
        "    # Accuracy\n",
        "    ACC = float(tp + tn) / test_num\n",
        "\n",
        "    # Sensitivity\n",
        "    if tp + fn == 0:\n",
        "        Recall = Sensitivity = 0\n",
        "    else:\n",
        "        Recall = Sensitivity = float(tp) / (tp + fn)\n",
        "\n",
        "    # Specificity\n",
        "    if tn + fp == 0:\n",
        "        Specificity = 0\n",
        "    else:\n",
        "        Specificity = float(tn) / (tn + fp)\n",
        "\n",
        "    # MCC\n",
        "    if (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) == 0:\n",
        "        MCC = 0\n",
        "    else:\n",
        "        MCC = float(tp * tn - fp * fn) / (np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)))\n",
        "\n",
        "    # ROC and AUC\n",
        "    FPR, TPR, thresholds = roc_curve(label_real, pred_prob, pos_label=1)\n",
        "\n",
        "    AUC = auc(FPR, TPR)\n",
        "\n",
        "    # PRC and AP\n",
        "    precision, recall, thresholds = precision_recall_curve(label_real, pred_prob, pos_label=1)\n",
        "    AP = average_precision_score(label_real, pred_prob, average='macro', pos_label=1, sample_weight=None)\n",
        "\n",
        "    performance = [ACC, Sensitivity, Specificity, AUC, MCC]\n",
        "    roc_data = [FPR, TPR, AUC]\n",
        "    prc_data = [recall, precision, AP]\n",
        "    return performance, roc_data, prc_data\n",
        "\n",
        "\n",
        "def to_log(log):\n",
        "    with open(\"./results/ExamPle_Log.log\", \"a+\") as f:\n",
        "        f.write(log + '\\n')"
      ],
      "metadata": {
        "id": "PmvuSop0ikq3"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = xAInet().to(device)\n",
        "lr = 0.0001\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "criterion = ContrastiveLoss()\n",
        "criterion_model = nn.CrossEntropyLoss(reduction='sum')\n",
        "best_acc = 0\n",
        "EPOCH = 5\n",
        "for epoch in range(EPOCH):\n",
        "    loss_ls = []\n",
        "    loss1_ls = []\n",
        "    loss2_3_ls = []\n",
        "    t0 = time.time()\n",
        "    net.train()\n",
        "    for seq1, seq2, label, label1, label2 in train_iter_cont:\n",
        "        output1 = net(seq1)\n",
        "        output2 = net(seq2)\n",
        "         \n",
        "        #pdb.set_trace()\n",
        "        output3 = net.train_model(seq1)\n",
        "\n",
        "        output4 = net.train_model(seq2)\n",
        "        loss1 = criterion(output1, output2, label)\n",
        "        loss2 = criterion_model(output3, label1)\n",
        "        loss3 = criterion_model(output4, label2)\n",
        "        loss = loss1 + loss2 + loss3\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_ls.append(loss.item())\n",
        "        loss1_ls.append(loss1.item())\n",
        "        loss2_3_ls.append((loss2 + loss3).item())\n",
        "\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        train_performance, train_roc_data, train_prc_data = evaluate(train_iter, net)\n",
        "        test_performance, test_roc_data, test_prc_data = evaluate(test_iter, net)\n",
        "\n",
        "    results = f\"\\nepoch: {epoch + 1}, loss: {np.mean(loss_ls):.5f}, loss1: {np.mean(loss1_ls):.5f}, loss2_3: {np.mean(loss2_3_ls):.5f}\\n\"\n",
        "    results += f'train_acc: {train_performance[0]:.4f}, time: {time.time() - t0:.2f}'\n",
        "    results += '\\n' + '=' * 16 + ' Test Performance. Epoch[{}] '.format(epoch + 1) + '=' * 16 \\\n",
        "               + '\\n[ACC,\\tSE,\\t\\tSP,\\t\\tAUC,\\tMCC]\\n' + '{:.4f},\\t{:.4f},\\t{:.4f},\\t{:.4f},\\t{:.4f}'.format(\n",
        "        test_performance[0], test_performance[1], test_performance[2], test_performance[3],\n",
        "        test_performance[4]) + '\\n' + '=' * 60\n",
        "    print(results)\n",
        "    # to_log(results)\n",
        "    test_acc = test_performance[0]  # test_performance: [ACC, Sensitivity, Specificity, AUC, MCC]\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        best_performance = test_performance\n",
        "        filename = '{}, {}[{:.3f}].pt'.format('ExamPle' + ', epoch[{}]'.format(epoch + 1), 'ACC', best_acc)\n",
        "        save_path_pt = os.path.join('./Model', filename)\n",
        "        # torch.save(net.state_dict(), save_path_pt, _use_new_zipfile_serialization=False)\n",
        "        best_results = '\\n' + '=' * 16 + colored(' Best Performance. Epoch[{}] ', 'red').format(epoch + 1) + '=' * 16 \\\n",
        "                       + '\\n[ACC,\\tSE,\\t\\tSP,\\t\\tAUC,\\tMCC]\\n' + '{:.4f},\\t{:.4f},\\t{:.4f},\\t{:.4f},\\t{:.4f}'.format(\n",
        "            best_performance[0], best_performance[1], best_performance[2], best_performance[3],\n",
        "            best_performance[4]) + '\\n' + '=' * 60\n",
        "        print(best_results)\n",
        "        best_ROC = test_roc_data\n",
        "        best_PRC = test_prc_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rBfcSfoiktp",
        "outputId": "72d42ca7-9e27-43d2-944f-42a35f7cbf9a"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 1, loss: 20.05409, loss1: 0.83335, loss2_3: 19.22074\n",
            "train_acc: 0.9757, time: 9.27\n",
            "================ Test Performance. Epoch[1] ================\n",
            "[ACC,\tSE,\t\tSP,\t\tAUC,\tMCC]\n",
            "0.9430,\t0.9617,\t0.9247,\t0.9855,\t0.8867\n",
            "============================================================\n",
            "\n",
            "================ Best Performance. Epoch[1] ================\n",
            "[ACC,\tSE,\t\tSP,\t\tAUC,\tMCC]\n",
            "0.9430,\t0.9617,\t0.9247,\t0.9855,\t0.8867\n",
            "============================================================\n",
            "\n",
            "epoch: 2, loss: 10.09637, loss1: 0.53051, loss2_3: 9.56586\n",
            "train_acc: 0.9852, time: 9.29\n",
            "================ Test Performance. Epoch[2] ================\n",
            "[ACC,\tSE,\t\tSP,\t\tAUC,\tMCC]\n",
            "0.9726,\t0.9660,\t0.9791,\t0.9903,\t0.9452\n",
            "============================================================\n",
            "\n",
            "================ Best Performance. Epoch[2] ================\n",
            "[ACC,\tSE,\t\tSP,\t\tAUC,\tMCC]\n",
            "0.9726,\t0.9660,\t0.9791,\t0.9903,\t0.9452\n",
            "============================================================\n",
            "\n",
            "epoch: 3, loss: 7.25422, loss1: 0.57480, loss2_3: 6.67942\n",
            "train_acc: 0.9979, time: 9.36\n",
            "================ Test Performance. Epoch[3] ================\n",
            "[ACC,\tSE,\t\tSP,\t\tAUC,\tMCC]\n",
            "0.9599,\t0.9787,\t0.9414,\t0.9935,\t0.9205\n",
            "============================================================\n",
            "\n",
            "epoch: 4, loss: 4.99697, loss1: 0.55507, loss2_3: 4.44190\n",
            "train_acc: 0.9984, time: 9.44\n",
            "================ Test Performance. Epoch[4] ================\n",
            "[ACC,\tSE,\t\tSP,\t\tAUC,\tMCC]\n",
            "0.9641,\t0.9830,\t0.9456,\t0.9938,\t0.9290\n",
            "============================================================\n",
            "\n",
            "epoch: 5, loss: 4.09475, loss1: 0.65421, loss2_3: 3.44053\n",
            "train_acc: 0.9958, time: 9.50\n",
            "================ Test Performance. Epoch[5] ================\n",
            "[ACC,\tSE,\t\tSP,\t\tAUC,\tMCC]\n",
            "0.9557,\t0.9830,\t0.9289,\t0.9947,\t0.9128\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(net, './model.pt')"
      ],
      "metadata": {
        "id": "j7hjeNzvoJ4m"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Load model"
      ],
      "metadata": {
        "id": "d9vN_c8BE2h6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('model.pt', map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "id": "egHhpJhEmsHh"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(file):\n",
        "    # Amino acid dictionary\n",
        "    aa_dict = {'A': 1, 'R': 2, 'N': 3, 'D': 4, 'C': 5, 'Q': 6, 'E': 7, 'G': 8, 'H': 9, 'I': 10,\n",
        "               'L': 11, 'K': 12, 'M': 13, 'F': 14, 'P': 15, 'O': 16, 'S': 17, 'U': 18, 'T': 19,\n",
        "               'W': 20, 'Y': 21, 'V': 22, 'X': 23}\n",
        "    \n",
        "    with open(file, 'r') as inf:\n",
        "        lines = inf.read().splitlines()\n",
        "\n",
        "    pep_codes = []\n",
        "    labels = []\n",
        "    peps = []\n",
        "    \n",
        "    for pep in lines:\n",
        "        pep, label = pep.split(\",\")\n",
        "        peps.append(pep)\n",
        "        labels.append(int(label))\n",
        "        current_pep = []\n",
        "        for aa in pep:\n",
        "            current_pep.append(aa_dict[aa])\n",
        "        pep_codes.append(torch.tensor(current_pep))\n",
        "\n",
        "        \n",
        "    desired_length = 299\n",
        "    padded_sequences = [seq[:desired_length] if len(seq) >= desired_length else torch.cat((seq, torch.zeros(desired_length - len(seq)))) for seq in pep_codes]\n",
        "# Apply pad_sequence on the padded_sequences\n",
        "    #data = pad_sequence(padded_sequences, batch_first=True)\n",
        "    \n",
        "    data = rnn_utils.pad_sequence(padded_sequences, batch_first=True)  # Fill the sequence to the same length\n",
        "  \n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "-kMIvoFmjmAu"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = generate_data('SSP_dataset.csv')"
      ],
      "metadata": {
        "id": "cMMZgCaYjnwO"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.to('cpu')\n",
        "model = model.to('cpu')"
      ],
      "metadata": {
        "id": "qX8o3gaxpU_h"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_output(inputs):\n",
        "\n",
        "    #inputs = inputs[0].unsqueeze(0)\n",
        "  \n",
        "    out = model(inputs)\n",
        "    # Apply softmax to convert prediction scores to probabilities\n",
        "    probabilities = torch.softmax(out, dim=1)\n",
        "    \n",
        "\n",
        "    # Get the predicted classes by selecting the class with the highest probability\n",
        "    predicted_classes = torch.argmax(probabilities, dim=1)  \n",
        "    return predicted_classes\n"
      ],
      "metadata": {
        "id": "VqZTILbTjpyb"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.long()"
      ],
      "metadata": {
        "id": "k7i_VnZev_oA"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_output(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R27fWY0DxZG1",
        "outputId": "567baff1-9324-4bfd-a746-9617a9f67e27"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 0, 1, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "xAI"
      ],
      "metadata": {
        "id": "7w3PmV8Rz2cZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install captum"
      ],
      "metadata": {
        "id": "evl0SV9izrPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import captum\n",
        "from captum.attr import LayerIntegratedGradients\n",
        "\n",
        "lig = LayerIntegratedGradients(model_output, model.embedding_layer)"
      ],
      "metadata": {
        "id": "D6ul5G_hz5Cj"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_input_and_baseline(text):\n",
        "\n",
        "    max_length = 512\n",
        "    #baseline_token_id = rnn_utils.pad_sequence()\n",
        "    \n",
        "\n",
        "    input_ids = []\n",
        "    token_list = []\n",
        "    \n",
        "    aa_dict = {'A': 1, 'R': 2, 'N': 3, 'D': 4, 'C': 5, 'Q': 6, 'E': 7, 'G': 8, 'H': 9, 'I': 10,\n",
        "               'L': 11, 'K': 12, 'M': 13, 'F': 14, 'P': 15, 'O': 16, 'S': 17, 'U': 18, 'T': 19,\n",
        "               'W': 20, 'Y': 21, 'V': 22, 'X': 23}\n",
        "    \n",
        "    for char in text:\n",
        "      if char in aa_dict:\n",
        "        input_ids.append(aa_dict[char])\n",
        "        token_list.append(char)\n",
        "\n",
        "    baseline_token_id = 23\n",
        "    baseline_input_ids = [baseline_token_id] * len(input_ids)\n",
        "\n",
        "    input_ids_tensor = torch.tensor([input_ids], device='cpu')\n",
        "    baseline_input_ids_tensor = torch.tensor([baseline_input_ids], device='cpu')\n",
        "\n",
        "    return input_ids_tensor, baseline_input_ids_tensor, token_list"
      ],
      "metadata": {
        "id": "3O-omLR4z9Fb"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'MSKSKMLVFKSKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKMSKSKMLVFKMSKSKMLVFKMSKSKMLVFKMSKSKMLVFK'\n",
        "\n",
        "input_ids, baseline_input_ids, all_tokens = construct_input_and_baseline(text)\n",
        "\n",
        "print(f'original text: {input_ids}')\n",
        "print(f'baseline text: {baseline_input_ids}')\n",
        "print(f'all tokens: {all_tokens}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtzD2T4J0Bli",
        "outputId": "28b50ac6-5516-4696-f8af-33d9a2f1423e"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text: tensor([[13, 17, 12, 17, 12, 13, 11, 22, 14, 12, 17, 12, 12, 12, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 17, 12, 17, 12, 13, 11, 22,\n",
            "         14, 12, 13, 17, 12, 17, 12, 13, 11, 22, 14, 12, 13, 17, 12, 17, 12, 13,\n",
            "         11, 22, 14, 12, 13, 17, 12, 17, 12, 13, 11, 22, 14, 12]])\n",
            "baseline text: tensor([[23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
            "         23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
            "         23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
            "         23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
            "         23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
            "         23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23]])\n",
            "all tokens: ['M', 'S', 'K', 'S', 'K', 'M', 'L', 'V', 'F', 'K', 'S', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'M', 'S', 'K', 'S', 'K', 'M', 'L', 'V', 'F', 'K', 'M', 'S', 'K', 'S', 'K', 'M', 'L', 'V', 'F', 'K', 'M', 'S', 'K', 'S', 'K', 'M', 'L', 'V', 'F', 'K', 'M', 'S', 'K', 'S', 'K', 'M', 'L', 'V', 'F', 'K']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "desired_length = 299\n",
        "padded_sequences = [seq[:desired_length] if len(seq) >= desired_length else torch.cat((seq, torch.zeros(desired_length - len(seq)))) for seq in input_ids]\n",
        "# Apply pad_sequence on the padded_sequences\n",
        "    #data = pad_sequence(padded_sequences, batch_first=True)\n",
        "    \n",
        "input_ids = rnn_utils.pad_sequence(padded_sequences, batch_first=True)"
      ],
      "metadata": {
        "id": "m2zoyXmb0Bqa"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desired_length = 299\n",
        "padded_sequences = [seq[:desired_length] if len(seq) >= desired_length else torch.cat((seq, torch.zeros(desired_length - len(seq)))) for seq in baseline_input_ids]\n",
        "# Apply pad_sequence on the padded_sequences\n",
        "    #data = pad_sequence(padded_sequences, batch_first=True)\n",
        "    \n",
        "baseline_input_ids = rnn_utils.pad_sequence(padded_sequences, batch_first=True)"
      ],
      "metadata": {
        "id": "ErPbJYQG0Bvy"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'original text: {input_ids}')\n",
        "print(f'baseline text: {baseline_input_ids}')\n",
        "print(f'all tokens: {all_tokens}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2X4kyKP2WFq",
        "outputId": "021c6dc6-bb08-4397-ab65-fa950828d080"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text: tensor([[13., 17., 12., 17., 12., 13., 11., 22., 14., 12., 17., 12., 12., 12.,\n",
            "         12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
            "         12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
            "         12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
            "         12., 12., 12., 12., 12., 12., 12., 12., 13., 17., 12., 17., 12., 13.,\n",
            "         11., 22., 14., 12., 13., 17., 12., 17., 12., 13., 11., 22., 14., 12.,\n",
            "         13., 17., 12., 17., 12., 13., 11., 22., 14., 12., 13., 17., 12., 17.,\n",
            "         12., 13., 11., 22., 14., 12.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.]])\n",
            "baseline text: tensor([[23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23.,\n",
            "         23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23.,\n",
            "         23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23.,\n",
            "         23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23.,\n",
            "         23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23.,\n",
            "         23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23.,\n",
            "         23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23., 23.,\n",
            "         23., 23., 23., 23., 23., 23.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.]])\n",
            "all tokens: ['M', 'S', 'K', 'S', 'K', 'M', 'L', 'V', 'F', 'K', 'S', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'M', 'S', 'K', 'S', 'K', 'M', 'L', 'V', 'F', 'K', 'M', 'S', 'K', 'S', 'K', 'M', 'L', 'V', 'F', 'K', 'M', 'S', 'K', 'S', 'K', 'M', 'L', 'V', 'F', 'K', 'M', 'S', 'K', 'S', 'K', 'M', 'L', 'V', 'F', 'K']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = input_ids.long()\n",
        "baseline_input_ids = baseline_input_ids.long()"
      ],
      "metadata": {
        "id": "c46XjsDP_H7F"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  attributions, delta = lig.attribute(inputs= input_ids,\n",
        "                                    baselines= baseline_input_ids,\n",
        "                                    return_convergence_delta=True,\n",
        "                                    internal_batch_size=1\n",
        "                                    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "K4PISaV528jO",
        "outputId": "fd41365f-183d-4768-880a-81ca462fbd8a"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-3f663f302e55>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   attributions, delta = lig.attribute(inputs= input_ids,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                     \u001b[0mbaselines\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbaseline_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                     \u001b[0mreturn_convergence_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                     \u001b[0minternal_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/captum/log/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/captum/attr/_core/layer/layer_integrated_gradients.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input)\u001b[0m\n\u001b[1;32m    492\u001b[0m         )\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m         attributions = self.ig.attribute.__wrapped__(  # type: ignore\n\u001b[0m\u001b[1;32m    495\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mig\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0minputs_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/captum/attr/_core/integrated_gradients.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minternal_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             attributions = _batch_attribution(\n\u001b[0m\u001b[1;32m    275\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mnum_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/captum/attr/_utils/batching.py\u001b[0m in \u001b[0;36m_batch_attribution\u001b[0;34m(attr_method, num_examples, internal_batch_size, n_steps, include_endpoint, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mstep_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_step_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0malphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_alphas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         current_attr = attr_method._attribute(\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_sizes_and_alphas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/captum/attr/_core/integrated_gradients.py\u001b[0m in \u001b[0;36m_attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         grads = self.gradient_func(\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mforward_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaled_features_tpl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/captum/attr/_core/layer/layer_integrated_gradients.py\u001b[0m in \u001b[0;36mgradient_func\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0;31m# torch.unbind(forward_out) is a list of scalar tensor tuples and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                 \u001b[0;31m# contains batch_size * #steps elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_vmap_internals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_none_pass_through\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "input_ids = input_ids.long()\n",
        "baseline_input_ids = baseline_input_ids.long()\n",
        "target=0\n",
        "\n",
        "\n",
        "ig = IntegratedGradients(model, model.embedding_layer)\n",
        "\n",
        "# Calcola le attribuzioni degli input_ids utilizzando gli Integrated Gradients\n",
        "attributions, delta = ig.attribute(input_id, targets)\n",
        "\n",
        "# Stampa le attribuzioni ottenute\n",
        "print(\"Attribuzioni degli input_ids:\", attributions)\n",
        "\n",
        "# Stampa la convergenza delta\n",
        "print(\"Convergenza delta:\", delta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "id": "MS5EM2GZAoua",
        "outputId": "3947292a-72d5-4cb6-b482-6d7816c66538"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-ad8c1dbd900a>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Calcola le attribuzioni degli input_ids utilizzando gli Integrated Gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mattributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Stampa le attribuzioni ottenute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/captum/log/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/captum/attr/_core/integrated_gradients.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[1;32m    284\u001b[0m             )\n\u001b[1;32m    285\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             attributions = self._attribute(\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mbaselines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaselines\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/captum/attr/_core/integrated_gradients.py\u001b[0m in \u001b[0;36m_attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         grads = self.gradient_func(\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mforward_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaled_features_tpl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/captum/_utils/gradient.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# runs forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         assert outputs[0].numel() == 1, (\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;34m\"Target not provided when necessary, cannot\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;34m\" take gradient with respect to multiple outputs.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Target not provided when necessary, cannot take gradient with respect to multiple outputs."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/francescopatane96/captum.git"
      ],
      "metadata": {
        "id": "GMMrV8DauZeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/francescopatane96/pytorch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWqhdlcj0v5m",
        "outputId": "fb6e249f-996c-4da2-f845-15ec23252877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/francescopatane96/pytorch.git\n",
            "  Cloning https://github.com/francescopatane96/pytorch.git to /tmp/pip-req-build-jpn1_rsd\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/francescopatane96/pytorch.git /tmp/pip-req-build-jpn1_rsd\n",
            "  Resolved https://github.com/francescopatane96/pytorch.git to commit f19535a948fa711e9fea7b9da5cd04043074dfc3\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "input_ids = input_ids.to(torch.long)\n",
        "baseline_input_ids = baseline_input_ids.to(torch.long)\n",
        "\n",
        "\n",
        "\n",
        "ig = IntegratedGradients(model_output, model.embedding_layer)"
      ],
      "metadata": {
        "id": "2Yns765fDfNp"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = model(input_ids[0:3])"
      ],
      "metadata": {
        "id": "7_jUnI3rG-2G"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHHbbTeawZ44",
        "outputId": "d5354765-830b-4bfd-ffc6-454b6547eccd"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.4333, -0.5113]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor(input_ids)\n",
        "baseline = torch.tensor(baseline_input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1L3pSdBHgOH",
        "outputId": "073e84af-64f7-47a9-e242-76084dd8c269"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-186-bee1fca2a0fd>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input = torch.tensor(input_ids)\n",
            "<ipython-input-186-bee1fca2a0fd>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  baseline = torch.tensor(baseline_input_ids)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ig = IntegratedGradients(model)\n",
        "\n",
        "\n",
        "attribution = ig.attribute(inputs = input, baselines=baseline, target=0)           #(, baselines = baseline, target=0)"
      ],
      "metadata": {
        "id": "TxA_f_87chba"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}