{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrU75eLR/RGeMzZzDQHWpf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescopatane96/protein-xAI/blob/main/see_me.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7CTzDdCOjClF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as Data\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "\n",
        "\n",
        "class xAInet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.hidden_dim = 25\n",
        "    self.batch_size = 32\n",
        "    self.embedding_dim = 512\n",
        "\n",
        "    self.embedding_layer = nn.Embedding(24, self.embedding_dim, padding_idx=0)\n",
        "    self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
        "\n",
        "    self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
        "    self.gru = nn.GRU(self.embedding_dim, self.hidden_dim, num_layers=2,\n",
        "                      bidirectional=True, dropout=.2)\n",
        "    \n",
        "    self.block_seq = nn.Sequential(nn.Linear(15050, 2048),\n",
        "                                   nn.BatchNorm1d(2048),\n",
        "                                   nn.LeakyReLU(),\n",
        "                                   nn.Linear(2048, 1024),\n",
        "                                   nn.BatchNorm1d(1024),\n",
        "                                   nn.LeakyReLU(),\n",
        "                                   nn.Linear(1024, 256),\n",
        "                                   nn.BatchNorm1d(256),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Linear(256, 8),\n",
        "                                   nn.Linear(8, 2),\n",
        "                                   nn.Softmax(dim=1))\n",
        "    \n",
        "    \n",
        "  def forward(self, seq):\n",
        "        seq = seq.long()\n",
        "        embeddings = self.embedding_layer(seq)\n",
        "        output = self.transformer_encoder(embeddings).permute(1, 0, 2)\n",
        "        output, hn = self.gru(output)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        hn = hn.permute(1, 0, 2)\n",
        "       \n",
        "        output = output.reshape(output.shape[0], -1)\n",
        "        hn = hn.reshape(output.shape[0], -1)\n",
        "        \n",
        "        output = torch.cat([output, hn], 1)\n",
        "        output = self.block_seq(output)\n",
        "       \n",
        "       \n",
        "        return output\n",
        "\n",
        "  def train_model(self, seq):\n",
        "    #with torch.no_grad():\n",
        "        output = self.forward(seq)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(file):\n",
        "    # Amino acid dictionary\n",
        "    aa_dict = {'A': 1, 'R': 2, 'N': 3, 'D': 4, 'C': 5, 'Q': 6, 'E': 7, 'G': 8, 'H': 9, 'I': 10,\n",
        "               'L': 11, 'K': 12, 'M': 13, 'F': 14, 'P': 15, 'O': 16, 'S': 17, 'U': 18, 'T': 19,\n",
        "               'W': 20, 'Y': 21, 'V': 22, 'X': 23}\n",
        "    \n",
        "    with open(file, 'r') as inf:\n",
        "        lines = inf.read().splitlines()\n",
        "\n",
        "    pep_codes = []\n",
        "    labels = []\n",
        "    peps = []\n",
        "    \n",
        "    for pep in lines:\n",
        "        pep, label = pep.split(\",\")\n",
        "        peps.append(pep)\n",
        "        labels.append(int(label))\n",
        "        current_pep = []\n",
        "        for aa in pep:\n",
        "            current_pep.append(aa_dict[aa])\n",
        "        pep_codes.append(torch.tensor(current_pep))\n",
        "\n",
        "        \n",
        "    desired_length = 299\n",
        "    padded_sequences = [seq[:desired_length] if len(seq) >= desired_length else torch.cat((seq, torch.zeros(desired_length - len(seq)))) for seq in pep_codes]\n",
        "# Apply pad_sequence on the padded_sequences\n",
        "    #data = pad_sequence(padded_sequences, batch_first=True)\n",
        "    \n",
        "    data = rnn_utils.pad_sequence(padded_sequences, batch_first=True)  # Fill the sequence to the same length\n",
        "  \n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "YEEXJTqAjwNI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_output(inputs):\n",
        "\n",
        "    #inputs = inputs[0].unsqueeze(0)\n",
        "  \n",
        "    out = model(inputs)\n",
        "    # Apply softmax to convert prediction scores to probabilities\n",
        "    probabilities = torch.softmax(out, dim=1)\n",
        "    \n",
        "\n",
        "    # Get the predicted classes by selecting the class with the highest probability\n",
        "    predicted_classes = torch.argmax(probabilities, dim=1)  \n",
        "    return predicted_classes\n"
      ],
      "metadata": {
        "id": "HLezKEJQj0th"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install captum"
      ],
      "metadata": {
        "id": "hlwi2CDOj2bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_input_and_baseline(text):\n",
        "\n",
        "    max_length = 512\n",
        "    #baseline_token_id = rnn_utils.pad_sequence()\n",
        "    \n",
        "\n",
        "    input_ids = []\n",
        "    token_list = []\n",
        "    \n",
        "    aa_dict = {'A': 1, 'R': 2, 'N': 3, 'D': 4, 'C': 5, 'Q': 6, 'E': 7, 'G': 8, 'H': 9, 'I': 10,\n",
        "               'L': 11, 'K': 12, 'M': 13, 'F': 14, 'P': 15, 'O': 16, 'S': 17, 'U': 18, 'T': 19,\n",
        "               'W': 20, 'Y': 21, 'V': 22, 'X': 23}\n",
        "    \n",
        "    for char in text:\n",
        "      if char in aa_dict:\n",
        "        input_ids.append(aa_dict[char])\n",
        "        token_list.append(char)\n",
        "\n",
        "    baseline_token_id = 23\n",
        "    baseline_input_ids = [baseline_token_id] * len(input_ids)\n",
        "\n",
        "    input_ids_tensor = torch.tensor([input_ids], device='cpu')\n",
        "    baseline_input_ids_tensor = torch.tensor([baseline_input_ids], device='cpu')\n",
        "\n",
        "    return input_ids_tensor, baseline_input_ids_tensor, token_list"
      ],
      "metadata": {
        "id": "KHaTjHNWj4UA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'MSKSKMLVFKSKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKMSKSKMLVFKMSKSKMLVFKMSKSKMLVFKMSKSKMLVFK'\n",
        "\n",
        "input_ids, baseline_input_ids, all_tokens = construct_input_and_baseline(text)\n",
        "\n",
        "print(f'original text: {input_ids}')\n",
        "print(f'baseline text: {baseline_input_ids}')\n",
        "print(f'all tokens: {all_tokens}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgi3rf3Pj6n-",
        "outputId": "0ed91ecc-4d3a-47c4-86b5-38922786586d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text: tensor([[13, 17, 12, 17, 12, 13, 11, 22, 14, 12, 17, 12, 12, 12, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 17, 12, 17, 12, 13, 11, 22,\n",
            "         14, 12, 13, 17, 12, 17, 12, 13, 11, 22, 14, 12, 13, 17, 12, 17, 12, 13,\n",
            "         11, 22, 14, 12, 13, 17, 12, 17, 12, 13, 11, 22, 14, 12]])\n",
            "baseline text: tensor([[23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
            "         23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
            "         23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
            "         23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
            "         23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
            "         23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23]])\n",
            "all tokens: ['M', 'S', 'K', 'S', 'K', 'M', 'L', 'V', 'F', 'K', 'S', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'M', 'S', 'K', 'S', 'K', 'M', 'L', 'V', 'F', 'K', 'M', 'S', 'K', 'S', 'K', 'M', 'L', 'V', 'F', 'K', 'M', 'S', 'K', 'S', 'K', 'M', 'L', 'V', 'F', 'K', 'M', 'S', 'K', 'S', 'K', 'M', 'L', 'V', 'F', 'K']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "desired_length = 299\n",
        "padded_sequences = [seq[:desired_length] if len(seq) >= desired_length else torch.cat((seq, torch.zeros(desired_length - len(seq)))) for seq in input_ids]\n",
        "# Apply pad_sequence on the padded_sequences\n",
        "    #data = pad_sequence(padded_sequences, batch_first=True)\n",
        "    \n",
        "input_ids = rnn_utils.pad_sequence(padded_sequences, batch_first=True)\n",
        "\n",
        "padded_sequences = [seq[:desired_length] if len(seq) >= desired_length else torch.cat((seq, torch.zeros(desired_length - len(seq)))) for seq in baseline_input_ids]\n",
        "# Apply pad_sequence on the padded_sequences\n",
        "    #data = pad_sequence(padded_sequences, batch_first=True)\n",
        "    \n",
        "baseline_input_ids = rnn_utils.pad_sequence(padded_sequences, batch_first=True)"
      ],
      "metadata": {
        "id": "wRmd-mDwj8f3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'original text: {input_ids}')\n",
        "print(f'baseline text: {baseline_input_ids}')\n",
        "print(f'all tokens: {all_tokens}')"
      ],
      "metadata": {
        "id": "qZoGpnwWkBht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = xAInet()\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "xOe_s4xTkeu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "ig = IntegratedGradients(model, model.embedding_layer)\n",
        "\n",
        "\n",
        "attribution = ig.attribute(inputs=input_ids, baselines=baseline_input_ids, target=model_output(input_ids))  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "dYQqJmAxkDPp",
        "outputId": "4b27bb13-0ae3-4288-984b-8d0bf3834673"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ec87c7096b18>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mattribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaseline_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/captum/log/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/captum/attr/_core/integrated_gradients.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[1;32m    284\u001b[0m             )\n\u001b[1;32m    285\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             attributions = self._attribute(\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mbaselines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaselines\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/captum/attr/_core/integrated_gradients.py\u001b[0m in \u001b[0;36m_attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         grads = self.gradient_func(\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mforward_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaled_features_tpl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/captum/_utils/gradient.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# torch.unbind(forward_out) is a list of scalar tensor tuples and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# contains batch_size * #steps elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_vmap_internals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_none_pass_through\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
          ]
        }
      ]
    }
  ]
}